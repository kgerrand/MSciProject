{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import config\n",
    "\n",
    "data_path = Path.home()/'OneDrive'/'Kirstin'/'Uni'/'Year4'/'MSciProject'/'data_files'/'saved_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploring sf6 at Gosan, South Korea.\n"
     ]
    }
   ],
   "source": [
    "site = config.site\n",
    "site_name = config.site_dict[site]\n",
    "compound = config.compound\n",
    "\n",
    "print(f\"Exploring {compound} at {site_name}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>flag</th>\n",
       "      <th>u10_0</th>\n",
       "      <th>u10_1</th>\n",
       "      <th>u10_2</th>\n",
       "      <th>u10_3</th>\n",
       "      <th>u10_4</th>\n",
       "      <th>u10_5</th>\n",
       "      <th>u10_6</th>\n",
       "      <th>u10_7</th>\n",
       "      <th>...</th>\n",
       "      <th>v500_2_past</th>\n",
       "      <th>v500_3_past</th>\n",
       "      <th>v500_4_past</th>\n",
       "      <th>v500_5_past</th>\n",
       "      <th>v500_6_past</th>\n",
       "      <th>v500_7_past</th>\n",
       "      <th>v500_8_past</th>\n",
       "      <th>v500_13_past</th>\n",
       "      <th>v500_14_past</th>\n",
       "      <th>v500_15_past</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2007-11-12 09:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.206935</td>\n",
       "      <td>2.064226</td>\n",
       "      <td>1.212830</td>\n",
       "      <td>1.286915</td>\n",
       "      <td>1.535215</td>\n",
       "      <td>-3.187690</td>\n",
       "      <td>-2.538291</td>\n",
       "      <td>-2.120406</td>\n",
       "      <td>...</td>\n",
       "      <td>-19.086115</td>\n",
       "      <td>-22.730377</td>\n",
       "      <td>-7.834412</td>\n",
       "      <td>-8.643053</td>\n",
       "      <td>-2.242004</td>\n",
       "      <td>-5.636626</td>\n",
       "      <td>4.728797</td>\n",
       "      <td>-1.143111</td>\n",
       "      <td>4.009740</td>\n",
       "      <td>0.372644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2007-11-12 11:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.428611</td>\n",
       "      <td>1.530584</td>\n",
       "      <td>2.358250</td>\n",
       "      <td>1.200097</td>\n",
       "      <td>-0.030984</td>\n",
       "      <td>-2.226324</td>\n",
       "      <td>-1.164828</td>\n",
       "      <td>-1.339043</td>\n",
       "      <td>...</td>\n",
       "      <td>-13.783958</td>\n",
       "      <td>-26.731780</td>\n",
       "      <td>-8.481802</td>\n",
       "      <td>-8.374302</td>\n",
       "      <td>-1.887253</td>\n",
       "      <td>-5.352347</td>\n",
       "      <td>2.065177</td>\n",
       "      <td>-0.155302</td>\n",
       "      <td>4.334629</td>\n",
       "      <td>1.973205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2007-11-12 13:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.483596</td>\n",
       "      <td>1.543896</td>\n",
       "      <td>2.329311</td>\n",
       "      <td>1.201254</td>\n",
       "      <td>-1.418916</td>\n",
       "      <td>-2.858360</td>\n",
       "      <td>-0.039087</td>\n",
       "      <td>-2.138927</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.764640</td>\n",
       "      <td>-26.449890</td>\n",
       "      <td>-10.537448</td>\n",
       "      <td>-8.391025</td>\n",
       "      <td>-2.325615</td>\n",
       "      <td>-2.430727</td>\n",
       "      <td>-0.247275</td>\n",
       "      <td>-0.398970</td>\n",
       "      <td>3.844906</td>\n",
       "      <td>2.299289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2007-11-12 15:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.732474</td>\n",
       "      <td>1.833869</td>\n",
       "      <td>1.425824</td>\n",
       "      <td>0.902021</td>\n",
       "      <td>-2.454367</td>\n",
       "      <td>-2.814372</td>\n",
       "      <td>0.606261</td>\n",
       "      <td>-2.136612</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.038415</td>\n",
       "      <td>-23.884214</td>\n",
       "      <td>-11.679340</td>\n",
       "      <td>-7.968189</td>\n",
       "      <td>-2.319643</td>\n",
       "      <td>-0.253247</td>\n",
       "      <td>-0.155302</td>\n",
       "      <td>-0.631887</td>\n",
       "      <td>2.570429</td>\n",
       "      <td>3.105542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2007-11-12 17:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.446553</td>\n",
       "      <td>1.407882</td>\n",
       "      <td>0.664718</td>\n",
       "      <td>1.079709</td>\n",
       "      <td>-3.140808</td>\n",
       "      <td>-2.384333</td>\n",
       "      <td>0.771794</td>\n",
       "      <td>-1.429334</td>\n",
       "      <td>...</td>\n",
       "      <td>-9.517389</td>\n",
       "      <td>-21.478596</td>\n",
       "      <td>-10.475337</td>\n",
       "      <td>-6.289989</td>\n",
       "      <td>-1.311528</td>\n",
       "      <td>2.177455</td>\n",
       "      <td>-1.407084</td>\n",
       "      <td>-1.211195</td>\n",
       "      <td>2.343484</td>\n",
       "      <td>2.292123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 150 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 time  flag     u10_0     u10_1     u10_2     u10_3     u10_4  \\\n",
       "0 2007-11-12 09:00:00   1.0 -0.206935  2.064226  1.212830  1.286915  1.535215   \n",
       "1 2007-11-12 11:00:00   1.0 -0.428611  1.530584  2.358250  1.200097 -0.030984   \n",
       "2 2007-11-12 13:00:00   1.0 -0.483596  1.543896  2.329311  1.201254 -1.418916   \n",
       "3 2007-11-12 15:00:00   1.0 -0.732474  1.833869  1.425824  0.902021 -2.454367   \n",
       "4 2007-11-12 17:00:00   1.0 -0.446553  1.407882  0.664718  1.079709 -3.140808   \n",
       "\n",
       "      u10_5     u10_6     u10_7  ...  v500_2_past  v500_3_past  v500_4_past  \\\n",
       "0 -3.187690 -2.538291 -2.120406  ...   -19.086115   -22.730377    -7.834412   \n",
       "1 -2.226324 -1.164828 -1.339043  ...   -13.783958   -26.731780    -8.481802   \n",
       "2 -2.858360 -0.039087 -2.138927  ...    -9.764640   -26.449890   -10.537448   \n",
       "3 -2.814372  0.606261 -2.136612  ...    -9.038415   -23.884214   -11.679340   \n",
       "4 -2.384333  0.771794 -1.429334  ...    -9.517389   -21.478596   -10.475337   \n",
       "\n",
       "   v500_5_past  v500_6_past  v500_7_past  v500_8_past  v500_13_past  \\\n",
       "0    -8.643053    -2.242004    -5.636626     4.728797     -1.143111   \n",
       "1    -8.374302    -1.887253    -5.352347     2.065177     -0.155302   \n",
       "2    -8.391025    -2.325615    -2.430727    -0.247275     -0.398970   \n",
       "3    -7.968189    -2.319643    -0.253247    -0.155302     -0.631887   \n",
       "4    -6.289989    -1.311528     2.177455    -1.407084     -1.211195   \n",
       "\n",
       "   v500_14_past  v500_15_past  \n",
       "0      4.009740      0.372644  \n",
       "1      4.334629      1.973205  \n",
       "2      3.844906      2.299289  \n",
       "3      2.570429      3.105542  \n",
       "4      2.343484      2.292123  \n",
       "\n",
       "[5 rows x 150 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "data = pd.read_csv(data_path/f'for_model_{compound}_{site}.csv', parse_dates=['time'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train range: 2009-01-01 01:00:00 -> 2013-12-30 09:00:00. Length: 5763\n",
      "Val range: 2014-01-03 19:00:00 -> 2014-12-31 23:00:00. Length: 933\n",
      "Test range: 2015-01-01 01:00:00 -> 2017-12-31 23:00:00. Length: 2824\n"
     ]
    }
   ],
   "source": [
    "if site == \"MHD\":\n",
    "    train_data = data[(data['time'].dt.year >= 2014) & (data['time'].dt.year <= 2018)]\n",
    "    val_data = data[(data['time'].dt.year >= 2019) & (data['time'].dt.year <= 2019)]\n",
    "    test_data = data[(data['time'].dt.year >= 2020) & (data['time'].dt.year <= 2023)]\n",
    "\n",
    "if site == \"GSN\":\n",
    "    train_data = data[(data['time'].dt.year >= 2009) & (data['time'].dt.year <= 2013)]\n",
    "    val_data = data[(data['time'].dt.year >= 2014) & (data['time'].dt.year <= 2014)]\n",
    "    test_data = data[(data['time'].dt.year >= 2015) & (data['time'].dt.year <= 2017)]\n",
    "\n",
    "print(f\"Train range: {train_data['time'].min()} -> {train_data['time'].max()}. Length: {len(train_data)}\")\n",
    "print(f\"Val range: {val_data['time'].min()} -> {val_data['time'].max()}. Length: {len(val_data)}\")\n",
    "print(f\"Test range: {test_data['time'].min()} -> {test_data['time'].max()}. Length: {len(test_data)}\")\n",
    "\n",
    "\n",
    "# Drop the \"time\" column as it won't be used in the model\n",
    "train_data = train_data.drop(columns=['time'])\n",
    "val_data = val_data.drop(columns=['time'])\n",
    "test_data = test_data.drop(columns=['time'])\n",
    "\n",
    "# Define the features (X) and the target (y)\n",
    "X_train = train_data.drop(columns=['flag'])\n",
    "y_train = train_data['flag']\n",
    "X_val = val_data.drop(columns=['flag'])\n",
    "y_val = val_data['flag']\n",
    "X_test = test_data.drop(columns=['flag'])\n",
    "y_test = test_data['flag']\n",
    "\n",
    "# Balanced Data - removing NaN values and associated data\n",
    "y_train = y_train.dropna()\n",
    "y_val = y_val.dropna()\n",
    "y_test = y_test.dropna()\n",
    "\n",
    "X_train = X_train.loc[y_train.index]\n",
    "X_val = X_val.loc[y_val.index]\n",
    "X_test = X_test.loc[y_test.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Default Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NN model with original parameters, evaluating based on performance on validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on Training Set = 0.850\n",
      "Precision on Validation Set = 0.778\n",
      "Recall on Training Set = 0.988\n",
      "Recall on Validation Set = 0.996\n",
      "F1 Score on Training Set = 0.914\n",
      "F1 Score on Validation Set = 0.873\n"
     ]
    }
   ],
   "source": [
    "# setting up a neural network model with default parameters\n",
    "nn_model = MLPClassifier(max_iter=1000, random_state=42)\n",
    "\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_val = nn_model.predict(X_val)\n",
    "y_pred_train = nn_model.predict(X_train)\n",
    "\n",
    "# calculating scores\n",
    "precision_val = precision_score(y_val, y_pred_val)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_val = recall_score(y_val, y_pred_val)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Precision on Training Set = {precision_train:.3f}\")\n",
    "print(f\"Precision on Validation Set = {precision_val:.3f}\")\n",
    "print(f\"Recall on Training Set = {recall_train:.3f}\")\n",
    "print(f\"Recall on Validation Set = {recall_val:.3f}\")\n",
    "print(f\"F1 Score on Training Set = {f1_train:.3f}\")\n",
    "print(f\"F1 Score on Validation Set = {f1_val:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on Test Set = 0.797\n",
      "Recall on Test Set = 0.992\n",
      "F1 Score on Test Set = 0.884\n"
     ]
    }
   ],
   "source": [
    "# evaluating the model on the test set\n",
    "y_pred_test = nn_model.predict(X_test)\n",
    "\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Precision on Test Set = {precision_test:.3f}\")\n",
    "print(f\"Recall on Test Set = {recall_test:.3f}\")\n",
    "print(f\"F1 Score on Test Set = {f1_test:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimising Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grid Search for Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation F1 Score: 0.405\n",
      "Best Parameters: {'activation': 'relu', 'alpha': 0.0001, 'batch_size': 100, 'early_stopping': False, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'max_iter': 1000, 'solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "model = MLPClassifier(random_state=42)\n",
    "\n",
    "# hyperparameters to explore\n",
    "param_grid = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'batch_size': [100, 200, 300],\n",
    "    'max_iter': [1000, 2000],\n",
    "    'early_stopping': [True, False]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(model, param_grid, n_jobs=-1, scoring='precision', cv=5)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# extracting best parameters and score\n",
    "results = grid_search.best_estimator_\n",
    "\n",
    "validation_f1 = results.score(X_val, y_val)\n",
    "\n",
    "print(f'Validation F1 Score: {validation_f1:.3f}')\n",
    "print(f'Best Parameters: {grid_search.best_params_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Optimised Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on Training Set = 0.905\n",
      "Precision on Testing Set = 0.879\n",
      "Recall on Training Set = 0.715\n",
      "Recall on Testing Set = 0.773\n",
      "F1 Score on Training Set = 0.799\n",
      "F1 Score on Testing Set = 0.823\n"
     ]
    }
   ],
   "source": [
    "nn_model = MLPClassifier(random_state=42,\n",
    "                         max_iter=1000, \n",
    "                         hidden_layer_sizes=(100,), \n",
    "                         shuffle=False,\n",
    "                         activation='relu', \n",
    "                         solver='adam', \n",
    "                         alpha=0.0001, \n",
    "                         learning_rate='constant', \n",
    "                         batch_size=100, \n",
    "                         early_stopping=False,\n",
    "                         learning_rate_init=0.0001,\n",
    "                         beta_2=0.9,)\n",
    "\n",
    "nn_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_val = nn_model.predict(X_val)\n",
    "y_pred_train = nn_model.predict(X_train)\n",
    "\n",
    "# calculating scores\n",
    "precision_val = precision_score(y_val, y_pred_val)\n",
    "precision_train = precision_score(y_train, y_pred_train)\n",
    "recall_val = recall_score(y_val, y_pred_val)\n",
    "recall_train = recall_score(y_train, y_pred_train)\n",
    "f1_val = f1_score(y_val, y_pred_val)\n",
    "f1_train = f1_score(y_train, y_pred_train)\n",
    "\n",
    "print(f\"Precision on Training Set = {precision_train:.3f}\")\n",
    "print(f\"Precision on Testing Set = {precision_val:.3f}\")\n",
    "print(f\"Recall on Training Set = {recall_train:.3f}\")\n",
    "print(f\"Recall on Testing Set = {recall_val:.3f}\")\n",
    "print(f\"F1 Score on Training Set = {f1_train:.3f}\")\n",
    "print(f\"F1 Score on Testing Set = {f1_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating Model on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on Testing Set = 0.890\n",
      "Recall on Testing Set = 0.770\n",
      "F1 Score on Testing Set = 0.826\n"
     ]
    }
   ],
   "source": [
    "y_pred_test = nn_model.predict(X_test)\n",
    "\n",
    "precision_test = precision_score(y_test, y_pred_test)\n",
    "recall_test = recall_score(y_test, y_pred_test)\n",
    "f1_test = f1_score(y_test, y_pred_test)\n",
    "\n",
    "print(f\"Precision on Testing Set = {precision_test:.3f}\")\n",
    "print(f\"Recall on Testing Set = {recall_test:.3f}\")\n",
    "print(f\"F1 Score on Testing Set = {f1_test:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\kirst\\\\OneDrive\\\\Kirstin\\\\Uni\\\\Year4\\\\MSciProject\\\\data_files\\\\saved_files\\\\nn_model.joblib']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# saving the model\n",
    "dump(nn_model, data_path/'nn_model.joblib')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
