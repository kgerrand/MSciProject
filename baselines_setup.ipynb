{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Setup for Baseline Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script sets up the variables used in the models. This involves defining the site and the compound being explored, extracting the meteorological data and reading and balancing the 'true' baselines (obtained from Alaistair Manning at the Met Office). The data is then combined to create a dataframe that is saved for use in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_path = Path.home()/'OneDrive'/'Kirstin'/'Uni'/'Year4'/'MSciProject'/'data_files'\n",
    "\n",
    "import functions as f\n",
    "\n",
    "site, site_name, compound = f.access_info()\n",
    "\n",
    "print(f\"Setting up data for \\033[1m{compound}\\033[0;0m at \\033[1m{site_name}\\033[0;0m.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up balancing\n",
    "minority_ratio = 0.8\n",
    "using_balanced = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting Manning's baseline flags for given site\n",
    "df = f.read_manning(site)\n",
    "\n",
    "# converting to xarray dataset\n",
    "ds_flags = df.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in meteorological data for given site\n",
    "# 10m wind\n",
    "ds_10m_u = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'10m_wind_grid').glob('*u*.nc'))\n",
    "ds_10m_v = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'10m_wind_grid').glob('*v*.nc'))\n",
    "\n",
    "# 850hPa wind\n",
    "ds_850hPa_u = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'850hPa_wind_grid').glob('*u*.nc'))\n",
    "ds_850hPa_v = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'850hPa_wind_grid').glob('*v*.nc'))\n",
    "\n",
    "# 500hPa wind\n",
    "ds_500hPa_u = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'500hPa_wind_grid').glob('*u*.nc'))\n",
    "ds_500hPa_v = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'500hPa_wind_grid').glob('*v*.nc'))\n",
    "\n",
    "# surface pressure\n",
    "ds_sp = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'surface_pressure').glob('*.nc'))\n",
    "\n",
    "# boundary layer height\n",
    "ds_blh = xr.open_mfdataset((data_path/'meteorological_data'/'ECMWF'/site/'boundary_layer_height').glob('*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the AGAGE data\n",
    "# original data\n",
    "# ds_agage = xr.open_dataset(data_path / f\"AGAGE/data-gcms-nc/AGAGE-GCMS-Medusa_{site}_{compound}.nc\")\n",
    "\n",
    "# reprocessed data\n",
    "ds_agage = xr.open_dataset(next((data_path / \"AGAGE\" / \"AGAGE-public-files\" / compound).glob(f\"*{site}_{compound}.nc\")))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an xarray dataset with all the meteorological data, the AGAGE data, and the baseline flags, based on the flags time index\n",
    "# adding a tolerance to the reindexing to allow for the AGAGE data to be reindexed to the nearest hour to avoid extrapolation of missing data\n",
    "data_ds = xr.merge([ds_flags,\n",
    "                    ds_10m_u.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_10m_v.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_850hPa_u.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_850hPa_v.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_500hPa_u.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_500hPa_v.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_sp.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_blh.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_agage.mf.reindex(time=ds_flags.time, method='nearest', tolerance=np.timedelta64(1, 'h'))],\n",
    "                    compat='override',\n",
    "                    combine_attrs='drop')\n",
    "\n",
    "data_ds = data_ds.drop_vars('level')\n",
    "\n",
    "# dropping any years after the final agage year\n",
    "agage_years = np.unique(ds_agage['time.year'])\n",
    "data_ds = data_ds.sel(time=data_ds['time.year'] <= agage_years[-1])\n",
    "\n",
    "data_ds.to_netcdf(data_path/'saved_files'/f'data_ds_{compound}_{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing the dataset\n",
    "balanced_data_ds = f.balance_baselines(data_ds, minority_ratio)\n",
    "\n",
    "# saving the balanced dataset\n",
    "balanced_data_ds.to_netcdf(data_path/'saved_files'/f'data_balanced_ds_{compound}_{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the mole fraction time series for the original and balanced datasets\n",
    "# calculating statistics\n",
    "original_mf = data_ds.mf.values\n",
    "original_mf = original_mf[~np.isnan(original_mf)]\n",
    "balanced_mf = balanced_data_ds.mf.values\n",
    "balanced_mf = balanced_mf[~np.isnan(balanced_mf)]\n",
    "\n",
    "original_mean = original_mf.mean()\n",
    "original_std = original_mf.std()\n",
    "balanced_mean = balanced_mf.mean()\n",
    "balanced_std = balanced_mf.std()\n",
    "\n",
    "print(f\"Original mean: {original_mean:.3f}, Balanced mean: {balanced_mean:.3f}. Percentage difference: {(abs(original_mean-balanced_mean)/original_mean)*100:.2f}%\"\n",
    "      f\"\\nOriginal std: {original_std:.3f}, Balanced std: {balanced_std:.3f}. Percentage difference: {(abs(original_std-balanced_std)/original_std)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the mole fraction time series for the balanced dataset\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "sns.set_theme(style='ticks', font='Arial')\n",
    "ax.minorticks_on()\n",
    "\n",
    "balanced_data_ds.mf.plot(ax=ax, label=\"All Data\", color='purple', alpha=0.5, linewidth=1)\n",
    "balanced_data_ds.where(balanced_data_ds.baseline == 1, drop=True).mf.plot(ax=ax, label=\"True Baselines\", color='darkgreen', linewidth=1.5)\n",
    "\n",
    "\n",
    "# ax.set_title(f\"{compound} Mole Fraction Time Series at {site_name}\", fontsize=15)\n",
    "ax.set_xlabel(\"\")\n",
    "ax.set_ylabel(\"mole fraction in air / ppt\", fontsize=12, fontstyle='italic')\n",
    "ax.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_balanced == False:\n",
    "    data_df = pd.DataFrame({\"flag\": data_ds.baseline.values}, index=data_ds.time.values)\n",
    "\n",
    "    points = data_ds.points.values\n",
    "\n",
    "    u10_columns = [f\"u10_{point}\" for point in points]\n",
    "    v10_columns = [f\"v10_{point}\" for point in points]\n",
    "    u850_columns = [f\"u850_{point}\" for point in points]\n",
    "    v850_columns = [f\"v850_{point}\" for point in points]\n",
    "    u500_columns = [f\"u500_{point}\" for point in points]\n",
    "    v500_columns = [f\"v500_{point}\" for point in points]\n",
    "\n",
    "    # concatenating the dataframe with the meteorological & temporal data\n",
    "    data_df = pd.concat([\n",
    "        data_df,\n",
    "        pd.DataFrame(data_ds.u10.sel(points=points).values, columns=u10_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.v10.sel(points=points).values, columns=v10_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.u850.sel(points=points).values, columns=u850_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.v850.sel(points=points).values, columns=v850_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.u500.sel(points=points).values, columns=u500_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.v500.sel(points=points).values, columns=v500_columns, index=data_df.index),\n",
    "        pd.DataFrame({\"sp\": data_ds.sp.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"blh\": data_ds.blh.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"time_of_day\": data_df.index.hour}, index=data_df.index),\n",
    "        pd.DataFrame({\"day_of_year\": data_df.index.dayofyear}, index=data_df.index)],\n",
    "        axis=1)\n",
    "    \n",
    "elif using_balanced == True:\n",
    "    data_df = pd.DataFrame({\"flag\": balanced_data_ds.baseline.values}, index=balanced_data_ds.time.values)\n",
    "\n",
    "    points = balanced_data_ds.points.values\n",
    "\n",
    "    u10_columns = [f\"u10_{point}\" for point in points]\n",
    "    v10_columns = [f\"v10_{point}\" for point in points]\n",
    "    u850_columns = [f\"u850_{point}\" for point in points]\n",
    "    v850_columns = [f\"v850_{point}\" for point in points]\n",
    "    u500_columns = [f\"u500_{point}\" for point in points]\n",
    "    v500_columns = [f\"v500_{point}\" for point in points]\n",
    "\n",
    "    # concatenating the dataframe with the meteorological & temporal data\n",
    "    data_df = pd.concat([\n",
    "        data_df,\n",
    "        pd.DataFrame(balanced_data_ds.u10.sel(points=points).values, columns=u10_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.v10.sel(points=points).values, columns=v10_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.u850.sel(points=points).values, columns=u850_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.v850.sel(points=points).values, columns=v850_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.u500.sel(points=points).values, columns=u500_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.v500.sel(points=points).values, columns=v500_columns, index=data_df.index),\n",
    "        pd.DataFrame({\"sp\": balanced_data_ds.sp.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"blh\": balanced_data_ds.blh.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"time_of_day\": data_df.index.hour}, index=data_df.index),\n",
    "        pd.DataFrame({\"day_of_year\": data_df.index.dayofyear}, index=data_df.index)],\n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = f.add_shifted_time(data_df, points)\n",
    "data_df.index.name = \"time\"\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option for saving the dataframe for the model before dimensionality reduction\n",
    "data_df.to_csv(data_path/'saved_files'/f\"for_model_{compound}_{site}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm reducing the dimensions of the dataset as has a huge number of features - can identify the most important features and remove noise.\n",
    "\n",
    "N/A for neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the flag column as it is the target variable - added back in later\n",
    "data_for_pca = data_df.drop(columns='flag')\n",
    "\n",
    "# dropping the temporal columns also\n",
    "# data_for_pca = data_for_pca.drop(columns=['time_of_day', 'day_of_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining column name lists to include shifted wind data\n",
    "u10_columns = [col for col in data_for_pca.columns if 'u10' in col]\n",
    "v10_columns = [col for col in data_for_pca.columns if 'v10' in col]\n",
    "u850_columns = [col for col in data_for_pca.columns if 'u850' in col]\n",
    "v850_columns = [col for col in data_for_pca.columns if 'v850' in col]\n",
    "u500_columns = [col for col in data_for_pca.columns if 'u500' in col]\n",
    "v500_columns = [col for col in data_for_pca.columns if 'v500' in col]\n",
    "\n",
    "wind_columns = u10_columns + v10_columns + u850_columns + v850_columns + u500_columns + v500_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardising the data for PCA based on column groups\n",
    "# groups = wind (groups by direction and height), sp, blh, time_of_day, day_of_year\n",
    "column_groups = {\n",
    "    'u10': u10_columns,\n",
    "    'v10': v10_columns,\n",
    "    'u850': u850_columns,\n",
    "    'v850': v850_columns,\n",
    "    'u500': u500_columns,\n",
    "    'v500': v500_columns,\n",
    "    'sp': ['sp'],\n",
    "    'blh': ['blh'],\n",
    "    'time_of_day': ['time_of_day'],\n",
    "    'day_of_year': ['day_of_year']\n",
    "}\n",
    "\n",
    "# creating dictionary to hold standardised data\n",
    "standardised_data = {}\n",
    "\n",
    "# standardising each group of columns\n",
    "for group, columns in column_groups.items():\n",
    "    data = data_for_pca[columns]\n",
    "    \n",
    "    # reshape if only one column - applicable for all but the wind columns\n",
    "    if data.shape[1] == 1:\n",
    "        data = data.values.reshape(-1, 1)\n",
    "    \n",
    "    standardised_data[group] = StandardScaler().fit_transform(data)\n",
    "\n",
    "\n",
    "# concatenating the standardised data into a dataframe for use in PCA\n",
    "# first converting the standardised data into dataframes so dimensions are correct for concatenation\n",
    "dfs = [pd.DataFrame(data, columns=columns) for group, columns, data in zip(column_groups.keys(), column_groups.values(), standardised_data.values())]\n",
    "\n",
    "# concatenating the dataframes\n",
    "data_for_pca = pd.concat(dfs, axis=1)\n",
    "\n",
    "data_for_pca.index = data_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_explained_variance = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the explained variance ratio for different numbers of components to find the optimal number of components\n",
    "# fitting the PCA with the standardised data\n",
    "pca = PCA()\n",
    "pca.fit(data_for_pca)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plotting the cumulative explained variance\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "ax.plot(cumulative_variance)\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Explained Variance')\n",
    "ax.set_title('Cumulative Explained Variance vs. Number of Components')\n",
    "\n",
    "# adding a horizontal line at the desired explained variance\n",
    "ax.axhline(y=desired_explained_variance, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "# annotating plot with the corresponding number of components\n",
    "desired_components = np.argmax(cumulative_variance >= desired_explained_variance) + 1\n",
    "ax.annotate(f'{desired_explained_variance*100:.0f}% Explained Variance = {desired_components} components',\n",
    "             xy=(desired_components, desired_explained_variance), xycoords='data',\n",
    "             xytext=(0, -30), textcoords='offset points',\n",
    "             arrowprops=dict(arrowstyle=\"->\", color='black'),\n",
    "             fontsize=9, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the PCA with the desired number of components (20)\n",
    "# num_components = desired_components\n",
    "num_components = 20\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# fitting the PCA with my data\n",
    "pca_data = pca.fit_transform(data_for_pca)\n",
    "pca_components = pd.DataFrame(pca_data, columns=[f\"PC{i+1}\" for i in range(num_components)], index=data_df.index)\n",
    "pca_components.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the loading for each component\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=[f\"PC{i+1}\" for i in range(num_components)], index=data_for_pca.columns)\n",
    "\n",
    "# saving the loadings to a csv file\n",
    "loadings.to_csv(data_path/'saved_files'/f\"pca_loadings_{compound}_{site}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a cumulative explained variance plot for the PCA components\n",
    "features = range(pca.n_components_)\n",
    "cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "ax.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "ax.step(range(1,(pca_components.shape[1]+1)), cum_var_exp, where='mid', label='Cumulative Explained Variance')\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Explained Variance')\n",
    "ax.set_title('Exploring the Explained Variance of PCA Components')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the flag column back in and saving the dataframe for use in the model\n",
    "pca_components['flag'] = data_df['flag']\n",
    "\n",
    "# adding back temporal variables\n",
    "# pca_components['time_of_day'] = data_df['time_of_day']\n",
    "# pca_components['day_of_year'] = data_df['day_of_year']\n",
    "\n",
    "# saving the PCA components dataframe\n",
    "pca_components.to_csv(data_path/'saved_files'/f'for_model_pca_{compound}_{site}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pd.read_csv(data_path/'saved_files'/f'for_model_pca_{compound}_{site}.csv', index_col=0)\n",
    "pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openghg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
