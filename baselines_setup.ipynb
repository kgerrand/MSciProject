{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Setup for Baseline Classification Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script sets up the variables used in the models. This involves defining the site and the compound being explored, extracting the meteorological data and reading and balancing the 'true' baselines (obtained from Alaistair Manning at the Met Office). The data is then combined to create a dataframe that is saved for use in the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import config\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data_path = Path.home()/'OneDrive'/'Kirstin'/'Uni'/'Year4'/'MSciProject'/'data_files'\n",
    "meteorological_data_path = data_path/'meteorological_data'\n",
    "saving_path = data_path/'saved_files'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up balancing\n",
    "minority_ratio = 0.8\n",
    "using_balanced = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = config.site\n",
    "site_name = config.site_dict[site]\n",
    "compound = config.compound\n",
    "\n",
    "print(f\"Setting up data for \\033[1m{compound}\\033[0;0m at \\033[1m{site_name}\\033[0;0m.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_manning(site):\n",
    "    \"\"\"\n",
    "    Extracting Alistair's baseline flags for a given site\n",
    "\n",
    "    Args:\n",
    "    - site (str): Site code (e.g., MHD)\n",
    "\n",
    "    Returns:\n",
    "    - df (pandas.DataFrame): DataFrame with baseline flags as a binary variable\n",
    "    \"\"\"\n",
    "    \n",
    "    site_translator = {\"MHD\":\"MH\", \"CGO\":\"CG\", \"GSN\":\"GS\", \"JFJ\":\"J1\", \"CMN\":\"M5\", \"THD\":\"TH\", \"ZEP\":\"ZE\"}\n",
    "\n",
    "    # Filtering so only including data relevant to the given site\n",
    "    files = (data_path / \"manning_baselines\").glob(f\"{site_translator[site]}*.txt\")\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    # Looping through each of the files for the given site\n",
    "    for file in files:\n",
    "\n",
    "        # Read the data, skipping metadata, putting into pandas dataframe\n",
    "        data = pd.read_csv(file, skiprows=6, delim_whitespace=True)\n",
    "\n",
    "        # Setting the index of the dataframe to be the extracted datetime and naming it time\n",
    "        data.index = pd.to_datetime(data['YY'].astype(str) + \"-\" + \\\n",
    "                                    data['MM'].astype(str) + \"-\" + \\\n",
    "                                    data['DD'].astype(str) + \" \" + \\\n",
    "                                    data['HH'].astype(str) + \":00:00\")\n",
    "\n",
    "        data.index.name = \"time\"\n",
    "        \n",
    "        # Adding the 'Ct' column to the previously created empty list\n",
    "        dfs.append(data[[\"Ct\"]])\n",
    "    \n",
    "    # Creating a dataframe from the list containing all the 'Ct' values\n",
    "    df = pd.concat(dfs)\n",
    "\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Replace all values in Ct column less than 10 or greater than 20 with 0\n",
    "    # not baseline values\n",
    "    df.loc[(df['Ct'] < 10) | (df['Ct'] >= 20), 'Ct'] = 0\n",
    "\n",
    "    # Replace all values between 10 and 19 with 1\n",
    "    # baseline values\n",
    "    df.loc[(df['Ct'] >= 10) & (df['Ct'] < 20), 'Ct'] = 1\n",
    "\n",
    "    # Rename Ct column to \"baseline\"\n",
    "    df.rename(columns={'Ct': 'baseline'}, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_baselines(ds, minority_ratio): \n",
    "    \"\"\"\n",
    "    Balances the dataset by randomly undersampling non-baseline data points.\n",
    "\n",
    "    Args:\n",
    "    - ds (xarray.Dataset): The dataset to be balanced.\n",
    "    - minority_ratio (float): The desired ratio of baseline (minority class) data points in the final dataset. \n",
    "                            For example, 0.4 means 40% of data points will be baseline.\n",
    "\n",
    "    Returns:\n",
    "    - xarray.Dataset: The balanced dataset where the ratio of baseline to non-baseline data points is as specified by the `minority_ratio` argument.\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If the counts of baseline and non-baseline values are not in the expected ratio (within a tolerance of 1%).\n",
    "\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # counting number of baseline&non-baseline data points\n",
    "    baseline_count = ds['baseline'].where(ds['baseline']==1).count()\n",
    "    non_baseline_count = ds['baseline'].where(ds['baseline']==0).count()\n",
    "    # print(f\"ORIGINAL baseline count: {baseline_count}, non-baseline count: {non_baseline_count}\")\n",
    "\n",
    "    # calculating the minority class count (expected to be baseline)\n",
    "    minority_count = int(min(baseline_count, non_baseline_count))\n",
    "\n",
    "    # calculating the majority class count based on majority_ratio and minority_count\n",
    "    majority_ratio = 1 - minority_ratio\n",
    "    majority_count = int(minority_count * (majority_ratio/minority_ratio))\n",
    "\n",
    "    # subsetting the non-baseline data points\n",
    "    undersampled_non_baseline = ds.where(ds['baseline'] == 0, drop=True)\n",
    "\n",
    "    # creating an array of time indices & randomly selecting some\n",
    "    time_indices = undersampled_non_baseline['time'].values\n",
    "    selected_indices = np.random.choice(time_indices, majority_count, replace=False)\n",
    "    selected_indices = np.sort(selected_indices)\n",
    "\n",
    "    # setting the non-baseline data points to only include the randomly selected indices\n",
    "    undersampled_non_baseline = undersampled_non_baseline.sel(time=selected_indices)\n",
    "\n",
    "    # combining the the undersampled non-baseline with the baseline values\n",
    "    balanced_ds = xr.merge([ds.sel(time=(ds['baseline'] == 1)), undersampled_non_baseline])\n",
    "    balanced_ds = balanced_ds.sortby('time')\n",
    "\n",
    "    # checking balance\n",
    "    new_baseline_count = balanced_ds['baseline'].where(balanced_ds['baseline']==1).count()\n",
    "    new_non_baseline_count = balanced_ds['baseline'].where(balanced_ds['baseline']==0).count()\n",
    "    # print(f\"NEW baseline count: {new_baseline_count}, non-baseline count: {new_non_baseline_count}\")\n",
    "\n",
    "    # verifying that the ratio of baseline:non-baseline data points is as expected (within a tolerance of 1%)\n",
    "    tolerance = 0.01\n",
    "    upper_bound = (1+tolerance)*(majority_ratio/minority_ratio)\n",
    "    lower_bound = (1-tolerance)*(majority_ratio/minority_ratio)\n",
    "\n",
    "    if(lower_bound <= (new_non_baseline_count/new_baseline_count) <= upper_bound):\n",
    "        return balanced_ds\n",
    "    else:\n",
    "        raise ValueError(\"The counts of baseline and non-baseline values are not in the expected ratio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shifted_time(df):\n",
    "    \"\"\"\n",
    "    Adds columns with wind data shifted by 6 hours (up three index rows) to the input dataframe.\n",
    "\n",
    "    Args:\n",
    "    - df (pd.DataFrame): The input dataframe.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: The dataframe with shifted time columns.\n",
    "    \"\"\"\n",
    "\n",
    "    # copying dataframe\n",
    "    df_ = df.copy()    \n",
    "\n",
    "    # extracting wind colunmns\n",
    "    u10_columns = [f\"u10_{point}\" for point in points]\n",
    "    v10_columns = [f\"v10_{point}\" for point in points]\n",
    "    u850_columns = [f\"u850_{point}\" for point in points]\n",
    "    v850_columns = [f\"v850_{point}\" for point in points]\n",
    "    u500_columns = [f\"u500_{point}\" for point in points]\n",
    "    v500_columns = [f\"v500_{point}\" for point in points]\n",
    "    wind_columns = u10_columns + v10_columns + u850_columns + v850_columns + u500_columns + v500_columns\n",
    "\n",
    "    # checking if adding a shifted time column has already been done - in which case, remove it before adding it again\n",
    "    if f'u10_0_past' in df_.columns:\n",
    "        df_ = df_.drop(columns=[col + f'_past' for col in wind_columns])\n",
    "        print(\"Shifted time columns already exist and have been removed. Note that redoing this function will remove additional columns.\")\n",
    "\n",
    "    # create shifted columns\n",
    "    shifted_columns = [col + '_past' for col in wind_columns]\n",
    "\n",
    "    # Create a dictionary for the shifted columns\n",
    "    shifted_dict = {}\n",
    "\n",
    "    for col, shifted_col in zip(wind_columns, shifted_columns):\n",
    "        # Shift the column values up by two rows\n",
    "        shifted_dict[shifted_col] = df_[col].shift(3)\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df_shifted = pd.DataFrame(shifted_dict)\n",
    "\n",
    "    # Concatenate the original DataFrame with the new DataFrame\n",
    "    df_ = pd.concat([df_, df_shifted], axis=1)\n",
    "\n",
    "    # dropping the first three rows as NaN values\n",
    "    df_ = df_.iloc[3:]\n",
    "\n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Up Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting Manning's baseline flags for given site\n",
    "df = read_manning(site)\n",
    "\n",
    "# converting to xarray dataset\n",
    "ds_flags = df.to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in meteorological data for given site\n",
    "# 10m wind\n",
    "ds_10m_u = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'10m_wind_grid').glob('*u*.nc'))\n",
    "ds_10m_v = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'10m_wind_grid').glob('*v*.nc'))\n",
    "\n",
    "# 850hPa wind\n",
    "ds_850hPa_u = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'850hPa_wind_grid').glob('*u*.nc'))\n",
    "ds_850hPa_v = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'850hPa_wind_grid').glob('*v*.nc'))\n",
    "\n",
    "# 500hPa wind\n",
    "ds_500hPa_u = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'500hPa_wind_grid').glob('*u*.nc'))\n",
    "ds_500hPa_v = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'500hPa_wind_grid').glob('*v*.nc'))\n",
    "\n",
    "# surface pressure\n",
    "ds_sp = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'surface_pressure').glob('*.nc'))\n",
    "\n",
    "# boundary layer height\n",
    "ds_blh = xr.open_mfdataset((meteorological_data_path/'ECMWF'/site/'boundary_layer_height').glob('*.nc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grabbing the AGAGE data\n",
    "# original data\n",
    "# ds_agage = xr.open_dataset(data_path / f\"AGAGE/data-gcms-nc/AGAGE-GCMS-Medusa_{site}_{compound}.nc\")\n",
    "\n",
    "# reprocessed data\n",
    "ds_agage = xr.open_dataset(next((data_path / \"AGAGE\" / \"AGAGE-public-files\" / compound).glob(f\"*{site}_{compound}.nc\")))                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating an xarray dataset with all the meteorological data, the AGAGE data, and the baseline flags, based on the flags time index\n",
    "# adding a tolerance to the reindexing to allow for the AGAGE data to be reindexed to the nearest hour to avoid extrapolation of missing data\n",
    "data_ds = xr.merge([ds_flags,\n",
    "                    ds_10m_u.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_10m_v.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_850hPa_u.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_850hPa_v.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_500hPa_u.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_500hPa_v.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_sp.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_blh.reindex(time=ds_flags.time, method='nearest'),\n",
    "                    ds_agage.mf.reindex(time=ds_flags.time, method='nearest', tolerance=np.timedelta64(1, 'h'))],\n",
    "                    compat='override',\n",
    "                    combine_attrs='drop')\n",
    "\n",
    "data_ds = data_ds.drop_vars('level')\n",
    "\n",
    "# dropping any years after the final agage year\n",
    "agage_years = np.unique(ds_agage['time.year'])\n",
    "data_ds = data_ds.sel(time=data_ds['time.year'] <= agage_years[-1])\n",
    "\n",
    "data_ds.to_netcdf(saving_path/f'data_ds_{compound}_{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing the dataset\n",
    "balanced_data_ds = balance_baselines(data_ds, minority_ratio)\n",
    "\n",
    "# saving the balanced dataset\n",
    "balanced_data_ds.to_netcdf(saving_path/f'data_balanced_ds_{compound}_{site}.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing the mole fraction time series for the original and balanced datasets\n",
    "# calculating statistics\n",
    "original_mf = data_ds.mf.values\n",
    "original_mf = original_mf[~np.isnan(original_mf)]\n",
    "balanced_mf = balanced_data_ds.mf.values\n",
    "balanced_mf = balanced_mf[~np.isnan(balanced_mf)]\n",
    "\n",
    "original_mean = original_mf.mean()\n",
    "original_std = original_mf.std()\n",
    "balanced_mean = balanced_mf.mean()\n",
    "balanced_std = balanced_mf.std()\n",
    "\n",
    "print(f\"Original mean: {original_mean:.3f}, Balanced mean: {balanced_mean:.3f}. Percentage difference: {(abs(original_mean-balanced_mean)/original_mean)*100:.2f}%\"\n",
    "      f\"\\nOriginal std: {original_std:.3f}, Balanced std: {balanced_std:.3f}. Percentage difference: {(abs(original_std-balanced_std)/original_std)*100:.2f}%\")\n",
    "\n",
    "# plotting\n",
    "fig, axs = plt.subplots(3,1, figsize=(12,12))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "data_ds.mf.plot(ax=axs[0], label=\"All Data\")\n",
    "data_ds.where(data_ds.baseline == 1, drop=True).mf.plot(ax=axs[0], label=\"Manning Baselines\")\n",
    "axs[0].set_title(\"Original Dataset\")\n",
    "axs[0].legend()\n",
    "\n",
    "balanced_data_ds.mf.plot(ax=axs[1], label=\"All Data\")\n",
    "balanced_data_ds.where(balanced_data_ds.baseline == 1, drop=True).mf.plot(ax=axs[1], label=\"Manning Baselines\")\n",
    "axs[1].set_title(\"Balanced Dataset\")\n",
    "axs[1].legend()\n",
    "\n",
    "data_ds.mf.plot(ax=axs[2], label=\"Original\")\n",
    "balanced_data_ds.mf.plot(ax=axs[2], label=\"Balanced\")\n",
    "axs[2].set_title(\"Both Datasets\")\n",
    "axs[2].legend()\n",
    "\n",
    "fig.suptitle(f\"{compound} at {site_name}\", fontsize=20)\n",
    "fig.set_tight_layout(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if using_balanced == False:\n",
    "    data_df = pd.DataFrame({\"flag\": data_ds.baseline.values}, index=data_ds.time.values)\n",
    "\n",
    "    points = data_ds.points.values\n",
    "\n",
    "    u10_columns = [f\"u10_{point}\" for point in points]\n",
    "    v10_columns = [f\"v10_{point}\" for point in points]\n",
    "    u850_columns = [f\"u850_{point}\" for point in points]\n",
    "    v850_columns = [f\"v850_{point}\" for point in points]\n",
    "    u500_columns = [f\"u500_{point}\" for point in points]\n",
    "    v500_columns = [f\"v500_{point}\" for point in points]\n",
    "\n",
    "    # concatenating the dataframe with the meteorological & temporal data\n",
    "    data_df = pd.concat([\n",
    "        data_df,\n",
    "        pd.DataFrame(data_ds.u10.sel(points=points).values, columns=u10_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.v10.sel(points=points).values, columns=v10_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.u850.sel(points=points).values, columns=u850_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.v850.sel(points=points).values, columns=v850_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.u500.sel(points=points).values, columns=u500_columns, index=data_df.index),\n",
    "        pd.DataFrame(data_ds.v500.sel(points=points).values, columns=v500_columns, index=data_df.index),\n",
    "        pd.DataFrame({\"sp\": data_ds.sp.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"blh\": data_ds.blh.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"time_of_day\": data_df.index.hour}, index=data_df.index),\n",
    "        pd.DataFrame({\"day_of_year\": data_df.index.dayofyear}, index=data_df.index)],\n",
    "        axis=1)\n",
    "    \n",
    "elif using_balanced == True:\n",
    "    data_df = pd.DataFrame({\"flag\": balanced_data_ds.baseline.values}, index=balanced_data_ds.time.values)\n",
    "\n",
    "    points = balanced_data_ds.points.values\n",
    "\n",
    "    u10_columns = [f\"u10_{point}\" for point in points]\n",
    "    v10_columns = [f\"v10_{point}\" for point in points]\n",
    "    u850_columns = [f\"u850_{point}\" for point in points]\n",
    "    v850_columns = [f\"v850_{point}\" for point in points]\n",
    "    u500_columns = [f\"u500_{point}\" for point in points]\n",
    "    v500_columns = [f\"v500_{point}\" for point in points]\n",
    "\n",
    "    # concatenating the dataframe with the meteorological & temporal data\n",
    "    data_df = pd.concat([\n",
    "        data_df,\n",
    "        pd.DataFrame(balanced_data_ds.u10.sel(points=points).values, columns=u10_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.v10.sel(points=points).values, columns=v10_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.u850.sel(points=points).values, columns=u850_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.v850.sel(points=points).values, columns=v850_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.u500.sel(points=points).values, columns=u500_columns, index=data_df.index),\n",
    "        pd.DataFrame(balanced_data_ds.v500.sel(points=points).values, columns=v500_columns, index=data_df.index),\n",
    "        pd.DataFrame({\"sp\": balanced_data_ds.sp.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"blh\": balanced_data_ds.blh.values}, index=data_df.index),\n",
    "        pd.DataFrame({\"time_of_day\": data_df.index.hour}, index=data_df.index),\n",
    "        pd.DataFrame({\"day_of_year\": data_df.index.dayofyear}, index=data_df.index)],\n",
    "        axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = add_shifted_time(data_df)\n",
    "data_df.index.name = \"time\"\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nan = data_df.columns[data_df.isna().any()]\n",
    "\n",
    "if columns_with_nan.size > 0:\n",
    "    print(f\"Columns containing NaN values: {columns_with_nan}\")\n",
    "\n",
    "    # dropping columns containing NaN values\n",
    "    data_df = data_df.dropna(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# option for saving the dataframe for the model before dimensionality reduction\n",
    "data_df.to_csv(saving_path/f\"for_model_{compound}_{site}.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm reducing the dimensions of the dataset as has a huge number of features - can identify the most important features and remove noise.\n",
    "\n",
    "N/A for neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the flag column as it is the target variable - added back in later\n",
    "data_for_pca = data_df.drop(columns='flag')\n",
    "\n",
    "# dropping the temporal columns also\n",
    "# data_for_pca = data_for_pca.drop(columns=['time_of_day', 'day_of_year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefining column name lists to include shifted wind data\n",
    "u10_columns = [col for col in data_for_pca.columns if 'u10' in col]\n",
    "v10_columns = [col for col in data_for_pca.columns if 'v10' in col]\n",
    "u850_columns = [col for col in data_for_pca.columns if 'u850' in col]\n",
    "v850_columns = [col for col in data_for_pca.columns if 'v850' in col]\n",
    "u500_columns = [col for col in data_for_pca.columns if 'u500' in col]\n",
    "v500_columns = [col for col in data_for_pca.columns if 'v500' in col]\n",
    "\n",
    "wind_columns = u10_columns + v10_columns + u850_columns + v850_columns + u500_columns + v500_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardising the data for PCA based on column groups\n",
    "# groups = wind (groups by direction and height), sp, blh, time_of_day, day_of_year\n",
    "column_groups = {\n",
    "    'u10': u10_columns,\n",
    "    'v10': v10_columns,\n",
    "    'u850': u850_columns,\n",
    "    'v850': v850_columns,\n",
    "    'u500': u500_columns,\n",
    "    'v500': v500_columns,\n",
    "    'sp': ['sp'],\n",
    "    'blh': ['blh'],\n",
    "    'time_of_day': ['time_of_day'],\n",
    "    'day_of_year': ['day_of_year']\n",
    "}\n",
    "\n",
    "# creating dictionary to hold standardised data\n",
    "standardised_data = {}\n",
    "\n",
    "# standardising each group of columns\n",
    "for group, columns in column_groups.items():\n",
    "    data = data_for_pca[columns]\n",
    "    \n",
    "    # reshape if only one column - applicable for all but the wind columns\n",
    "    if data.shape[1] == 1:\n",
    "        data = data.values.reshape(-1, 1)\n",
    "    \n",
    "    standardised_data[group] = StandardScaler().fit_transform(data)\n",
    "\n",
    "\n",
    "# concatenating the standardised data into a dataframe for use in PCA\n",
    "# first converting the standardised data into dataframes so dimensions are correct for concatenation\n",
    "dfs = [pd.DataFrame(data, columns=columns) for group, columns, data in zip(column_groups.keys(), column_groups.values(), standardised_data.values())]\n",
    "\n",
    "# concatenating the dataframes\n",
    "data_for_pca = pd.concat(dfs, axis=1)\n",
    "\n",
    "data_for_pca.index = data_df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Fitting PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_explained_variance = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploring the explained variance ratio for different numbers of components to find the optimal number of components\n",
    "# fitting the PCA with the standardised data\n",
    "pca = PCA()\n",
    "pca.fit(data_for_pca)\n",
    "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plotting the cumulative explained variance\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "ax.plot(cumulative_variance)\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Cumulative Explained Variance')\n",
    "ax.set_title('Cumulative Explained Variance vs. Number of Components')\n",
    "\n",
    "# adding a horizontal line at the desired explained variance\n",
    "ax.axhline(y=desired_explained_variance, color='r', linestyle='--', alpha=0.7)\n",
    "\n",
    "# annotating plot with the corresponding number of components\n",
    "desired_components = np.argmax(cumulative_variance >= desired_explained_variance) + 1\n",
    "ax.annotate(f'{desired_explained_variance*100:.0f}% Explained Variance = {desired_components} components',\n",
    "             xy=(desired_components, desired_explained_variance), xycoords='data',\n",
    "             xytext=(0, -30), textcoords='offset points',\n",
    "             arrowprops=dict(arrowstyle=\"->\", color='black'),\n",
    "             fontsize=9, color='black')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the PCA with the desired number of components (20)\n",
    "# num_components = desired_components\n",
    "num_components = 20\n",
    "pca = PCA(n_components=num_components)\n",
    "\n",
    "# fitting the PCA with my data\n",
    "pca_data = pca.fit_transform(data_for_pca)\n",
    "pca_components = pd.DataFrame(pca_data, columns=[f\"PC{i+1}\" for i in range(num_components)], index=data_df.index)\n",
    "pca_components.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieving the loading for each component\n",
    "loadings = pd.DataFrame(pca.components_.T, columns=[f\"PC{i+1}\" for i in range(num_components)], index=data_for_pca.columns)\n",
    "\n",
    "# saving the loadings to a csv file\n",
    "loadings.to_csv(saving_path/f\"pca_loadings_{compound}_{site}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting a cumulative explained variance plot for the PCA components\n",
    "features = range(pca.n_components_)\n",
    "cum_var_exp = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "ax.bar(features, pca.explained_variance_ratio_, color='black')\n",
    "ax.step(range(1,(pca_components.shape[1]+1)), cum_var_exp, where='mid', label='Cumulative Explained Variance')\n",
    "ax.set_xlabel('Number of Components')\n",
    "ax.set_ylabel('Explained Variance')\n",
    "ax.set_title('Exploring the Explained Variance of PCA Components')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the flag column back in and saving the dataframe for use in the model\n",
    "pca_components['flag'] = data_df['flag']\n",
    "\n",
    "# adding back temporal variables\n",
    "# pca_components['time_of_day'] = data_df['time_of_day']\n",
    "# pca_components['day_of_year'] = data_df['day_of_year']\n",
    "\n",
    "# saving the PCA components dataframe\n",
    "pca_components.to_csv(saving_path/f'for_model_pca_{compound}_{site}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = pd.read_csv(saving_path/f'for_model_pca_{compound}_{site}.csv', index_col=0)\n",
    "pca"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openghg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
